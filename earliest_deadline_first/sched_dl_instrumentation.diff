diff --git a/kernel/sched_dl.c b/kernel/sched_dl.c
index 9d0443e..a228b88 100644
--- a/kernel/sched_dl.c
+++ b/kernel/sched_dl.c
@@ -14,6 +14,10 @@
  *                    Fabio Checconi <fabio@gandalf.sssup.it>
  */
 
+static long long nsec_high(unsigned long long nsec);
+static unsigned long nsec_low(unsigned long long nsec);
+#define SPLIT_NS(x) nsec_high(x), nsec_low(x)
+
 static inline int dl_time_before(u64 a, u64 b)
 {
 	return (s64)(a - b) < 0;
@@ -69,6 +73,9 @@ static inline void setup_new_dl_entity(struct sched_dl_entity *dl_se)
 	dl_se->deadline = rq->clock + dl_se->dl_deadline;
 	dl_se->runtime = dl_se->dl_runtime;
 	dl_se->dl_new = 0;
+
+	printk("[%d] gets new BW pair; d_s at %Ld.%06ld\n",
+	       dl_task_of(dl_se)->pid, SPLIT_NS(dl_se->deadline));
 }
 
 /*
@@ -118,6 +125,16 @@ static void replenish_dl_entity(struct sched_dl_entity *dl_se)
 		WARN_ON_ONCE(1);
 		dl_se->deadline = rq->clock + dl_se->dl_deadline;
 		dl_se->runtime = dl_se->dl_runtime;
+
+		printk("[%d] postpones its deadline but it lags too much;"
+		       " d_l is set to %Ld.%06ld;"
+		       " c_s is fully topped up\n",
+		       dl_task_of(dl_se)->pid, SPLIT_NS(dl_se->deadline));
+	} else {
+		printk("[%d] postpones its deadline to %Ld.%06ld;"
+		       " c_s is topped up to %Ld.%06ld\n",
+		       dl_task_of(dl_se)->pid,
+		       SPLIT_NS(dl_se->deadline), SPLIT_NS(dl_se->runtime));
 	}
 }
 
@@ -132,7 +149,8 @@ static void replenish_dl_entity(struct sched_dl_entity *dl_se)
  */
 static bool dl_entity_overflow(struct sched_dl_entity *dl_se, u64 t)
 {
-	u64 left, right;
+	u64 left, right, remaining_time = dl_se->deadline - t;
+	int right_before_left;
 
 	/*
 	 * left and right are the two sides of the equation above,
@@ -148,9 +166,17 @@ static bool dl_entity_overflow(struct sched_dl_entity *dl_se, u64 t)
 	 * type is very unlikely to occur in both cases.
 	 */
 	left = dl_se->dl_deadline * dl_se->runtime;
-	right = (dl_se->deadline - t) * dl_se->dl_runtime;
+	right = remaining_time * dl_se->dl_runtime;
 
-	return dl_time_before(right, left);
+	right_before_left = dl_time_before(right, left);
+
+	printk("[%d] %Ld.%06ld / %Ld.%06ld %s %Ld.%06ld / %Ld.%06ld\n",
+	       dl_task_of(dl_se)->pid,
+	       SPLIT_NS(dl_se->runtime), SPLIT_NS(remaining_time),
+	       right_before_left ? ">" : "<=",
+	       SPLIT_NS(dl_se->dl_runtime), SPLIT_NS(dl_se->dl_deadline));
+
+	return right_before_left;
 }
 
 /*
@@ -180,6 +206,11 @@ static void update_dl_entity(struct sched_dl_entity *dl_se)
 	    dl_entity_overflow(dl_se, rq->clock)) {
 		dl_se->deadline = rq->clock + dl_se->dl_deadline;
 		dl_se->runtime = dl_se->dl_runtime;
+		printk("[%d] d_s is renewed (r + D) to %Ld.%06ld;"
+		       " c_s is fully topped up\n",
+		       dl_task_of(dl_se)->pid, SPLIT_NS(dl_se->deadline));
+	} else {
+		printk("[%d] resumes its BW pair\n", dl_task_of(dl_se)->pid);
 	}
 }
 
@@ -261,6 +292,8 @@ static enum hrtimer_restart dl_task_timer(struct hrtimer *timer)
 	if (!dl_task(p))
 		goto unlock;
 
+	printk("[%d] Hard CBS timer goes off\n", p->pid);
+
 	dl_se->dl_throttled = 0;
 	if (p->se.on_rq) {
 		enqueue_task_dl(rq, p, ENQUEUE_REPLENISH);
@@ -336,6 +369,7 @@ static void update_curr_dl(struct rq *rq)
 
 	dl_se->runtime -= delta_exec;
 	if (dl_runtime_exceeded(rq, dl_se)) {
+		printk("[%d] uses up its c_s\n", dl_task_of(dl_se)->pid);
 		__dequeue_task_dl(rq, curr, 0);
 		if (likely(start_dl_timer(dl_se)))
 			dl_se->dl_throttled = 1;
@@ -427,9 +461,12 @@ static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 	 * its rq, the bandwidth timer callback (which clearly has not
 	 * run yet) will take care of this.
 	 */
-	if (p->dl.dl_throttled)
+	if (p->dl.dl_throttled) {
+		printk("[%d] cannot be enqueued: throttled\n", p->pid);
 		return;
+	}
 
+	printk("[%d] is enqueued\n", p->pid);
 	enqueue_dl_entity(&p->dl, flags);
 }
 
@@ -440,6 +477,7 @@ static void __dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 
 static void dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 {
+	printk("[%d] is dequeued\n", p->pid);
 	update_curr_dl(rq);
 	__dequeue_task_dl(rq, p, flags);
 }
@@ -453,6 +491,7 @@ static void dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 static void yield_task_dl(struct rq *rq)
 {
 	struct task_struct *p = rq->curr;
+	printk("[%d] yield_task_dl\n", p->pid);
 
 	/*
 	 * We make the task go to sleep until its current deadline by
@@ -472,7 +511,11 @@ static void check_preempt_curr_dl(struct rq *rq, struct task_struct *p,
 {
 	if (!dl_task(rq->curr) || (dl_task(p) &&
 	    dl_time_before(p->dl.deadline, rq->curr->dl.deadline)))
+	{
 		resched_task(rq->curr);
+		printk("[%d] preempts %s (%d)\n",
+		       p->pid, p->comm, rq->curr->pid);
+	}
 }
 
 #ifdef CONFIG_SCHED_HRTICK
@@ -520,11 +563,14 @@ struct task_struct *pick_next_task_dl(struct rq *rq)
 	if (hrtick_enabled(rq))
 		start_hrtick_dl(rq, p);
 #endif
+	printk("[%d] is pick_next_task_dl-ed by %s (%d)\n",
+	       p->pid, current->comm, current->pid);
 	return p;
 }
 
 static void put_prev_task_dl(struct rq *rq, struct task_struct *p)
 {
+	printk("[%d] put_prev_task_dl\n", p->pid);
 	update_curr_dl(rq);
 	p->se.exec_start = 0;
 }
@@ -541,6 +587,7 @@ static void task_tick_dl(struct rq *rq, struct task_struct *p, int queued)
 
 static void task_fork_dl(struct task_struct *p)
 {
+	printk("[%d] task_fork_dl\n", p->pid);
 	/*
 	 * The child of a -deadline task will be SCHED_DEADLINE, but
 	 * as a throttled task. This means the parent (or someone else)
@@ -553,6 +600,7 @@ static void task_fork_dl(struct task_struct *p)
 
 static void task_dead_dl(struct task_struct *p)
 {
+	printk("[%d] task_dead_dl\n", p->pid);
 	/*
 	 * We are not holding any lock here, so it is safe to
 	 * wait for the bandwidth timer to be removed.
@@ -564,12 +612,14 @@ static void set_curr_task_dl(struct rq *rq)
 {
 	struct task_struct *p = rq->curr;
 
+	printk("[%d] set_curr_task_dl\n", p->pid);
 	p->se.exec_start = rq->clock;
 }
 
 static void switched_from_dl(struct rq *rq, struct task_struct *p,
 			     int running)
 {
+	printk("[%d] switched_from_dl\n", p->pid);
 	if (hrtimer_active(&p->dl.dl_timer))
 		hrtimer_try_to_cancel(&p->dl.dl_timer);
 }
@@ -577,21 +627,29 @@ static void switched_from_dl(struct rq *rq, struct task_struct *p,
 static void switched_to_dl(struct rq *rq, struct task_struct *p,
 			   int running)
 {
+	printk("[%d] switched_to_dl", p->pid);
 	/*
 	 * If p is throttled, don't consider the possibility
 	 * of preempting rq->curr, the check will be done right
 	 * after its runtime will get replenished.
 	 */
-	if (unlikely(p->dl.dl_throttled))
+	if (unlikely(p->dl.dl_throttled)) {
+		printk(", but throttled\n");
 		return;
+	}
 
-	if (!running)
+	if (!running) {
+		printk(", and not running");
 		check_preempt_curr_dl(rq, p, 0);
+	}
+
+	printk("\n");
 }
 
 static void prio_changed_dl(struct rq *rq, struct task_struct *p,
 			    int oldprio, int running)
 {
+	printk("[%d] prio_changed_dl\n", p->pid);
 	switched_to_dl(rq, p, running);
 }
 
@@ -599,6 +657,7 @@ static void prio_changed_dl(struct rq *rq, struct task_struct *p,
 static int
 select_task_rq_dl(struct rq *rq, struct task_struct *p, int sd_flag, int flags)
 {
+	printk("[%d] select_task_rq_dl\n", p->pid);
 	return task_cpu(p);
 }
 
@@ -609,6 +668,7 @@ static void set_cpus_allowed_dl(struct task_struct *p,
 
 	BUG_ON(!dl_task(p));
 
+	printk("[%d] set_cpus_allowed_dl\n", p->pid);
 	cpumask_copy(&p->cpus_allowed, new_mask);
 	p->dl.nr_cpus_allowed = weight;
 }
