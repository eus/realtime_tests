diff --git a/kernel/sched.c b/kernel/sched.c
index 67d545c..bc1ec53 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -703,8 +703,15 @@ static inline struct task_group *task_group(struct task_struct *p)
 
 inline void update_rq_clock(struct rq *rq)
 {
-	if (!rq->skip_clock_update)
+	if (!rq->skip_clock_update) {
+		u64 old_clock = rq->clock;
 		rq->clock = sched_clock_cpu(cpu_of(rq));
+		trace_printk("rq->clock is %s\n",
+			     old_clock == rq->clock ? "not updated" : "updated"
+			);
+	} else {
+		trace_printk("rq->clock update is skipped\n");
+	}
 }
 
 /*
diff --git a/kernel/sched_dl.c b/kernel/sched_dl.c
index 70ce1d3..0e35331 100644
--- a/kernel/sched_dl.c
+++ b/kernel/sched_dl.c
@@ -74,6 +74,9 @@ static inline void setup_new_dl_entity(struct sched_dl_entity *dl_se)
 	dl_se->deadline = rq->clock + dl_se->dl_deadline;
 	dl_se->runtime = dl_se->dl_runtime;
 	dl_se->dl_new = 0;
+
+	trace_printk("@%llu [%d] gets new BW pair; d_s at %llu\n",
+		     rq->clock, dl_task_of(dl_se)->pid, dl_se->deadline);
 }
 
 /*
@@ -123,6 +126,18 @@ static void replenish_dl_entity(struct sched_dl_entity *dl_se)
 		WARN_ON_ONCE(1);
 		dl_se->deadline = rq->clock + dl_se->dl_deadline;
 		dl_se->runtime = dl_se->dl_runtime;
+
+		trace_printk("@%llu [%d] postpones its deadline"
+			     " but it lags too much;"
+			     " d_l is set to %llu;"
+			     " c_s is fully topped up\n", rq->clock,
+			     dl_task_of(dl_se)->pid, dl_se->deadline);
+	} else {
+		trace_printk("@%llu [%d] postpones its deadline to %llu;"
+			     " c_s is topped up to %llu\n",
+			     rq->clock, dl_task_of(dl_se)->pid,
+			     dl_se->deadline, dl_se->runtime
+			);
 	}
 }
 
@@ -137,7 +152,8 @@ static void replenish_dl_entity(struct sched_dl_entity *dl_se)
  */
 static bool dl_entity_overflow(struct sched_dl_entity *dl_se, u64 t)
 {
-	u64 left, right;
+	u64 left, right, remaining_time = dl_se->deadline - t;
+	int right_before_left;
 
 	/*
 	 * left and right are the two sides of the equation above,
@@ -153,9 +169,18 @@ static bool dl_entity_overflow(struct sched_dl_entity *dl_se, u64 t)
 	 * type is very unlikely to occur in both cases.
 	 */
 	left = dl_se->dl_deadline * dl_se->runtime;
-	right = (dl_se->deadline - t) * dl_se->dl_runtime;
+	right = remaining_time * dl_se->dl_runtime;
+
+	right_before_left = dl_time_before(right, left);
 
-	return dl_time_before(right, left);
+	trace_printk("@%llu [%d] %llu / %llu %s %llu / %llu\n",
+		     rq_of_dl_rq(dl_rq_of_se(dl_se))->clock,
+		     dl_task_of(dl_se)->pid,
+		     dl_se->runtime, remaining_time,
+		     right_before_left ? ">" : "<=",
+		     dl_se->dl_runtime, dl_se->dl_deadline);
+
+	return right_before_left;
 }
 
 /*
@@ -185,6 +210,12 @@ static void update_dl_entity(struct sched_dl_entity *dl_se)
 	    dl_entity_overflow(dl_se, rq->clock)) {
 		dl_se->deadline = rq->clock + dl_se->dl_deadline;
 		dl_se->runtime = dl_se->dl_runtime;
+		trace_printk("@%llu [%d] d_s is renewed (r + D) to %llu;"
+			     " c_s is fully topped up\n", rq->clock,
+			     dl_task_of(dl_se)->pid, dl_se->deadline);
+	} else {
+		trace_printk("@%llu [%d] resumes its BW pair\n", rq->clock,
+			     dl_task_of(dl_se)->pid);
 	}
 }
 
@@ -266,6 +297,8 @@ static enum hrtimer_restart dl_task_timer(struct hrtimer *timer)
 	if (!dl_task(p))
 		goto unlock;
 
+	trace_printk("@%llu [%d] Hard CBS timer goes off\n", rq->clock, p->pid);
+
 	dl_se->dl_throttled = 0;
 	if (p->se.on_rq) {
 		enqueue_task_dl(rq, p, ENQUEUE_REPLENISH);
@@ -309,6 +342,9 @@ int dl_runtime_exceeded(struct rq *rq, struct sched_dl_entity *dl_se)
 		dl_se->runtime = rorun ? dl_se->runtime : 0;
 		dl_se->runtime -= rq->clock - dl_se->deadline;
 	}
+	trace_printk("@%llu [%d]%s%s\n", rq->clock, dl_task_of(dl_se)->pid,
+		     rorun ? " rorun" : "",
+		     dmiss ? " dmiss" : "");
 
 	return 1;
 }
@@ -322,6 +358,7 @@ static void update_curr_dl(struct rq *rq)
 	struct task_struct *curr = rq->curr;
 	struct sched_dl_entity *dl_se = &curr->dl;
 	u64 delta_exec;
+	u64 prev_rq_clock, prev_exec_start;
 
 	if (!dl_task(curr) || !on_dl_rq(dl_se))
 		return;
@@ -330,6 +367,9 @@ static void update_curr_dl(struct rq *rq)
 	if (unlikely((s64)delta_exec < 0))
 		delta_exec = 0;
 
+	prev_rq_clock = rq->clock;
+	prev_exec_start = curr->se.exec_start;
+
 	schedstat_set(curr->se.statistics.exec_max,
 		      max(curr->se.statistics.exec_max, delta_exec));
 
@@ -341,6 +381,11 @@ static void update_curr_dl(struct rq *rq)
 
 	dl_se->runtime -= delta_exec;
 	if (dl_runtime_exceeded(rq, dl_se)) {
+
+		trace_printk("@%llu [%d] uses up its c_s; %llu-%llu=%llu\n",
+			     rq->clock, curr->pid,
+			     prev_rq_clock, prev_exec_start, delta_exec);
+
 		__dequeue_task_dl(rq, curr, 0);
 		if (likely(start_dl_timer(dl_se)))
 			dl_se->dl_throttled = 1;
@@ -432,9 +477,13 @@ static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 	 * its rq, the bandwidth timer callback (which clearly has not
 	 * run yet) will take care of this.
 	 */
-	if (p->dl.dl_throttled)
+	if (p->dl.dl_throttled) {
+		trace_printk("@%llu [%d] cannot be enqueued: throttled\n",
+			     rq->clock, p->pid);
 		return;
+	}
 
+	trace_printk("@%llu [%d] is enqueued\n", rq->clock, p->pid);
 	enqueue_dl_entity(&p->dl, flags);
 }
 
@@ -445,6 +494,7 @@ static void __dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 
 static void dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 {
+	trace_printk("@%llu [%d] is dequeued\n", rq->clock, p->pid);
 	update_curr_dl(rq);
 	__dequeue_task_dl(rq, p, flags);
 }
@@ -458,6 +508,7 @@ static void dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 static void yield_task_dl(struct rq *rq)
 {
 	struct task_struct *p = rq->curr;
+	trace_printk("@%llu [%d] yield_task_dl\n", rq->clock, p->pid);
 
 	/*
 	 * We make the task go to sleep until its current deadline by
@@ -477,7 +528,11 @@ static void check_preempt_curr_dl(struct rq *rq, struct task_struct *p,
 {
 	if (!dl_task(rq->curr) || (dl_task(p) &&
 	    dl_time_before(p->dl.deadline, rq->curr->dl.deadline)))
+	{
 		resched_task(rq->curr);
+		trace_printk("@%llu [%d] preempts %s (%d)\n", rq->clock,
+			     p->pid, rq->curr->comm, rq->curr->pid);
+	}
 }
 
 #ifdef CONFIG_SCHED_HRTICK
@@ -527,11 +582,14 @@ struct task_struct *pick_next_task_dl(struct rq *rq)
 	if (hrtick_enabled(rq))
 		start_hrtick_dl(rq, p);
 #endif
+	trace_printk("@%llu [%d] is pick_next_task_dl-ed by %s (%d)\n",
+		     rq->clock, p->pid, current->comm, current->pid);
 	return p;
 }
 
 static void put_prev_task_dl(struct rq *rq, struct task_struct *p)
 {
+	trace_printk("@%llu [%d] put_prev_task_dl\n", rq->clock, p->pid);
 	update_curr_dl(rq);
 	p->se.exec_start = 0;
 }
@@ -548,6 +606,8 @@ static void task_tick_dl(struct rq *rq, struct task_struct *p, int queued)
 
 static void task_fork_dl(struct task_struct *p)
 {
+	trace_printk("@%llu [%d] task_fork_dl\n",
+		     rq_of_dl_rq(dl_rq_of_se(&p->dl))->clock, p->pid);
 	/*
 	 * The child of a -deadline task will be SCHED_DEADLINE, but
 	 * as a throttled task. This means the parent (or someone else)
@@ -560,6 +620,8 @@ static void task_fork_dl(struct task_struct *p)
 
 static void task_dead_dl(struct task_struct *p)
 {
+	trace_printk("@%llu [%d] task_dead_dl\n",
+		     rq_of_dl_rq(dl_rq_of_se(&p->dl))->clock, p->pid);
 	/*
 	 * We are not holding any lock here, so it is safe to
 	 * wait for the bandwidth timer to be removed.
@@ -571,12 +633,14 @@ static void set_curr_task_dl(struct rq *rq)
 {
 	struct task_struct *p = rq->curr;
 
+	trace_printk("@%llu [%d] set_curr_task_dl\n", rq->clock, p->pid);
 	p->se.exec_start = rq->clock;
 }
 
 static void switched_from_dl(struct rq *rq, struct task_struct *p,
 			     int running)
 {
+	trace_printk("@%llu [%d] switched_from_dl\n", rq->clock, p->pid);
 	if (hrtimer_active(&p->dl.dl_timer))
 		hrtimer_try_to_cancel(&p->dl.dl_timer);
 }
@@ -589,16 +653,23 @@ static void switched_to_dl(struct rq *rq, struct task_struct *p,
 	 * of preempting rq->curr, the check will be done right
 	 * after its runtime will get replenished.
 	 */
-	if (unlikely(p->dl.dl_throttled))
+	if (unlikely(p->dl.dl_throttled)) {
+		trace_printk("@%llu [%d] switched_to_dl, but throttled\n",
+			     rq->clock, p->pid);
 		return;
+	}
 
-	if (!running)
+	if (!running) {
+		trace_printk("@%llu [%d] switched_to_dl, and not running\n",
+			     rq->clock, p->pid);
 		check_preempt_curr_dl(rq, p, 0);
+	}
 }
 
 static void prio_changed_dl(struct rq *rq, struct task_struct *p,
 			    int oldprio, int running)
 {
+	trace_printk("@%llu [%d] prio_changed_dl\n", rq->clock, p->pid);
 	switched_to_dl(rq, p, running);
 }
 
@@ -606,6 +677,7 @@ static void prio_changed_dl(struct rq *rq, struct task_struct *p,
 static int
 select_task_rq_dl(struct rq *rq, struct task_struct *p, int sd_flag, int flags)
 {
+	trace_printk("@%llu [%d] select_task_rq_dl\n", rq->clock, p->pid);
 	return task_cpu(p);
 }
 
@@ -616,6 +688,7 @@ static void set_cpus_allowed_dl(struct task_struct *p,
 
 	BUG_ON(!dl_task(p));
 
+	trace_printk("[%d] set_cpus_allowed_dl\n", p->pid);
 	cpumask_copy(&p->cpus_allowed, new_mask);
 	p->dl.nr_cpus_allowed = weight;
 }
