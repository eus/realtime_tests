		Constant Bandwidth Server Inheritance
		   for Client-Server Communication
----------------------------------------------------------------------

This experimentation unit requires the Linux kernel found in
git://gitorious.org/sched_deadline/linux-deadline.git
in branch latest/2.6.36-dl-V3 at commit
f2ebcfd122cdab46f1e9eabe91e1549285b5a00b.

This experiment consists of several sub-experiments. All
sub-experiments are concerned about BWI for client-server
communication, such as RPC (Remote Procedure Call) or the
communication between an X server and its clients.

All sub-experiments have their parameters stored in .cfg files to be
run by the driver program: main. For example, to run sub-experiment 1
for 60 seconds, the following command can be used:

./main -t 60s -p 20 -f subexperiment_01.cfg

The result will be file subexperiment_*.bin that can be read using
read_task_stats_file executable. In particular, the cummulative
distribution frequency (CDF) plot of the client response time can be
obtained by running cdf.m obtained by a command like the following in
GNU Octave:
../read_task_stats_file -c matlab subexperiment_01.bin > cdf.m

To obtain a two columns file suitable for plotting using gnuplot, a
command like the following can be used instead:
../read_task_stats_file -c gnuplot subexperiment_01.bin > cdf.gpl

As a convenient, the BASH script run_experiment.sh takes a
sub-experiment number and produces both the .bin and .m files.

[Sub-experiment 1]
First, a server accepting and responding to UDP packet and a CPU
hogger performing an infinite loop are run.
Then, a client task is run using a CBS server whose Q_s is 20 ms and
T_s is 30 ms (i.e., the CPU utilitization U_s is 63%) mimicking a
multimedia application that has to render 30 frames per second.

The client task itself consists of three consecutive parts:
1. Prologue that runs for 5 ms if it monopolizes the CPU.
2. Service  that runs for 9 ms if it monopolizes the CPU.
3. Epilogue that runs for 5 ms if it monopolizes the CPU.
------------------------------------------------------- +
With a total duration of 19 ms if it monopolizes the CPU.

Even though the total computational needed is 19 ms, Q_s is set at 20
ms to take into account the kernel tick whose finest granularity is 1
ms (HZ=1000).

The second part, however, happens in the server. To initiate the
second part, the client task sends a UDP packet to the server before
blocking waiting for the server respond.

The expected CDF graph of the client's response time is a slope after
19 ms as opposed to a vertical line at 19 ms.

[Sub-experiment 2]
Same as sub-experiment 1 except that the second part of the client is
done by the server under BWI inheriting the client's CBS server.

The expected CDF graph of the client's response time is a vertical
line at 19 ms.

[Sub-experiment 3]
Same as sub-experiment 1 but an additional CPU hogger is run using a
CBS server whose Q_s is 5 ms (the kernel tick is not taken into
account because this CBS server will run an infinite loop) and T_s is
30 ms such that its bandwidth is 16.67%. When the SCHED_DEADLINE
kernel is using the original CBS replenishment behavior (work
conserving mode), the CBS CPU hogger will prevent the server from
being run at all, and therefore, the client will only be able to
complete one prologue-service-epilogue that is late. When the
SCHED_DEADLINE kernel is using the hard CBS replenishment behavior
(non-work conserving mode), the client would have a larger response
time compared to the one obtained in sub-experiment 1.

Running "read_task_stats_file subexperiment_03.bin" will show only
one late completed job in work conserving mode.
In non-work conserving mode, the expected CDF graph of the client's
response time is a less steeper slope after 19 ms compared to the CDF
graph of sub-experiment 1.

[Sub-experiment 4]
Same as sub-experiment 3 except that the second part of the client is
done by the server under BWI inheriting the client's CBS server.

The expected CDF graph of the client's response time is a vertical
line at 19 ms.

[Sub-experiment 5]
Same as sub-experiment 3 but instead of a CPU hogger, a HRT CBS is
used instead (a HRT CBS never postpones its deadline). The kernel tick
is taken into account by setting the HRT CBS budget to 6 ms. The HRT
CBS period is set to 20 ms to make it preempt the client when BWI is
used making the effect of HRT CBS on the client response time
visible. As documented in
subexperiment_06-hrt_cbs_period_*-{client,hrt_cbs}_first.png, using
HRT CBS period 30, 40 and 50 do not ensure that HRT CBS will always
preempt the client both when the client starts first and when the HRT
CBS starts first.

The expected CDF graph of the client's response time is a slope after
19 ms.

[Sub-experiment 6]
Same as sub-experiment 5 except that the second part of the client is
done by the server under BWI inheriting the client's CBS server.

The expected CDF graph of the client's response time is a vertical
line at 19 ms and 24 ms because there is a competing HRT server that
always preempts the client half of the time as can be seen in
subexperiment_06-hrt_cbs_period_20-{client,hrt_cbs}_first.png. Nevertheless,
the CDF should end at a time t less than or equal to 30 ms.

[Sub-experiment 7]
Same as sub-experiment 1 except that the server is run using a CBS
whose Q_s is 4 ms and T_s is 30 ms. The kernel tick is not taken into
account because the server will surely need more than 4 ms.

The expected CDF graph of the client's response time is a vertical
line at 19 ms because the CPU hogger cannot interfere with the server
that runs using a CBS.

[Sub-experiment 8]
Same as sub-experiment 7 except that an additional CPU hogger is run
using a CBS whose Q_s is 6 ms and T_s is 30 ms. The kernel tick is not
taken into account because the CBS will run an infinite loop.

The expected CDF graph of the client's response time is a slope
after 19 ms because the server CBS execution will frequently be
interrupted by the CPU hogger running using a CBS.

* NOTE: When the CBS of the CPU hogger has Q_s = 5 ms, the CDF graph
  is a vertical line at 19 ms. It seems that Q_s = 5 ms is a "lucky"
  number that gets along well with the server CBS.

[Sub-experiment 9]
Same as sub-experiment 8 except that BWI is in use.

The expected CDF graph of the client's response time is a vertical
line at 19 ms because the server CBS has more execution time using
client CBS.

[Sub-experiment 10]
Same as sub-experiment 8 except that the additional CPU hogger running
using a CBS is replaced by a periodic task whose WCET is 5 ms running
inside CBS whose budget is set to 6 ms to take into account the finest
kernel tick and whose period is 30 ms.

The expected CDF graph of the client's response time is a vertical
line at 25 ms based on the following execution time line:

. means arrival
d=x means the deadline associated with a particular arrival
.-- execution starts from the arrival time
|-- execution starts after the arrival time
--( execution is transferred to the server
)-- execution is transferred from the server
--| execution ends before the deadline
t=x means a particular point in time

client  .     |----(        )----|
              d=30               t=25
server             .---.---.|
                   d=41 d=71 d=101
HRT     .-----|
        d=30

[Sub-experiment 11]
Same as sub-experiment 10 except that BWI is in use.

The expected CDF graph of the client's response time should be similar
to that that of sub-experiment 10 because no deadline is ever
postponed.

[Sub-experiment 12]
Same as sub-experiment 1 except that an additional client keeps
requesting the server service one after another. Since the server
queue is FIFO and the additional client only sends the next request
after the server sends the response, the CBS client has a worst-case
blocking time of 9 ms when using BWI. Unfortunately, in this
sub-experiment, BWI is not in use, and therefore, the CBS client will
surely be late.

The blocking time of 9 ms to be experienced by the client must be
taken into account by increasing the client CBS budget to 29 ms (19 ms
of computation time and 9 ms of blocking time and 1 ms due to kernel
tick)

The expected CDF graph of the client's response time is a slope after
19 ms.

[Sub-experiment 13]
Same as sub-experiment 12 except that BWI is in use.

The expected CDF graph of the client's response time is a slope
between 19 ms and 28 ms due to 0 ms best-case blocking time and 9 ms
worst-case blocking time.

[Sub-experiment 14]
Same as sub-experiment 1 except that the client CBS budget is set to
10 ms (11 ms taking into account the finest kernel tick) instead of 19
ms.

The expected CDF graph of the client's response time is a slope
after 19 ms.

[Sub-experiment 15]
Same as sub-experiment 14 except that BWI is in use.

The expected CDF graph of the client's response time is a vertical
line at 19 ms assuming work conserving mode is in use because the
client CBS can exploit the slack time.

[Sub-experiment 16]
Same as sub-experiment 15 except that a HRT CBS is also run with Q_s =
5 ms (set to 6 ms to take into account the finest kernel tick) and
T_s = 30 ms.

The expected CDF graph of the client's response time is a vertical
line at 24 ms due to the competition with CBS whose budget is 5 ms,
assuming work conserving mode.

[Sub-experiment 17]
Same as sub-experiment 7 except that the client CBS budget is set to
10 ms (11 ms actually to take into account the kernel tick). This is
because without BWI, the client only needs to perform 10 ms of
computation (5 ms prologue and 5 ms epilogue). Moreover, without BWI,
a budget of 19 ms will lower, not raise, the chance for the client to
complete its computation within 30 ms when the server can execute
without interference, which is the case in sub-experiment 7 due to the
server using its own CBS. Specifically, when the client using CBS with
budget 19 ms sends request to the server after executing its prologue
for 5 ms, the client remaining budget is 14 ms. When the server sends
its response after executing without interference for 9 ms, the client
CBS moves from being idle to active at time r with remaining budget 14
ms but the distance to the deadline is 30 - 5 - 9 = 16 ms. Since 14/16
is greater than CBS bandwidth 19/30, the CBS will generate a new
deadline 30 ms away from r lowering the client chance to complete its
epilogue within 30 ms since the prologue starts. With a lower budget
of 10 ms, the remaining budget at time r will 5 ms with the same
distance to deadline, which is 16 ms. Since 5/16 is lower than the
bandwidth 10/30, the client has a higher chance to complete its
epilogue within 30 ms since the prologue starts. This trick of
lowering the budget to raise the client chance to complete in time
has the opposite effect of lowering the client chance when BWI is in
use because the client CBS budget keeps being consumed under BWI.

[Sub-experiment 18]
Same as sub-experiment 17 except that an additional CPU hogger is run
using a CBS whose Q_s is 6 ms and T_s is 30 ms. Kernel tick is not
taken into account because the CBS will run an infinite loop.

Same as sub-experiment 8 except that the client CBS budget is set to
10 ms (11 ms actually to take into account the kernel tick).

[Sub-experiment 19]
Same as sub-experiment 18 except that BWI is in use.

Same as sub-experiment 9 except that the client CBS budget is set to
10 ms (11 ms actually to take into account the kernel tick).

[Sub-experiment 20]
Same as sub-experiment 17 except that the server CBS budget is set to
9 ms (10 ms actually to take into account the kernel tick).

[Sub-experiment 21]
Same as sub-experiment 18 except that the server CBS budget is set to
9 ms (10 ms actually to take into account the kernel tick).

[Sub-experiment 22]
Same as sub-experiment 19 except that the server CBS budget is set to
9 ms (10 ms actually to take into account the kernel tick).

[Sub-experiment 23]
Same as sub-experiment 21 except that the CPU hogger CBS budget is set
to 5 ms and the period to 20 ms.

[Sub-experiment 24]
Same as sub-experiment 23 except that the CPU hogger CBS is replaced
by HRT CBS with the same budget and period but whose HRT task WCET is
4 ms to take into account the finest kernel tick.
